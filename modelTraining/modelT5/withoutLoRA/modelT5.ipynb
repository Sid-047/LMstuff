{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sid\\AppData\\Local\\Temp\\ipykernel_30028\\401704351.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msiddiqmoideen07\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>t:\\OpenSourceContribution\\LMstuff\\modelTraining\\modelT5\\withoutLoRA\\wandb\\run-20240531_233050-rlgmy3du</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21/runs/rlgmy3du' target=\"_blank\">astral-butterfly-5</a></strong> to <a href='https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21' target=\"_blank\">https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21/runs/rlgmy3du' target=\"_blank\">https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21/runs/rlgmy3du</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/siddiqmoideen07/T5%20Model%20FineTuning%20Yo%21/runs/rlgmy3du?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c92414af40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"T5 Model FineTuning Yo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OCR Contents</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invoice Number: INV0786Hotel Name: Spice Garde...</td>\n",
       "      <td>Food Category - Restaurant Bill/Receipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invoice No- DW/1112/851166\\n\\nInvoice Date : 2...</td>\n",
       "      <td>Travel Category - Flight Ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hotel: Accor\\r\\nCity: Delhi\\r\\nNumber of Days:...</td>\n",
       "      <td>Miscellaneous Category - Miscellaneous Bills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOOD INN FINE DINE\\nProprietor\\nPER =NNIA REST...</td>\n",
       "      <td>Food Category - Restaurant Bill/Receipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Confirmed Booking Id: NF20115276689422\\n\\nSU...</td>\n",
       "      <td>Travel Category - Flight Ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>Indiandit\\n\\nSERIAL SNA AUTONOBI\\n\\nr .A.B.Roa...</td>\n",
       "      <td>Fuel Category - Fuel Bill/Receipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5162</th>\n",
       "      <td>Base Fare\\nDistance\\nTime\\n\\nSubtotal\\n\\nCHARG...</td>\n",
       "      <td>Travel Category - Cab Bill/Receipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5163</th>\n",
       "      <td>Dear sanjay kumar,\\n\\nCongratdatons! Thark you...</td>\n",
       "      <td>Travel Category - Train Ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>Invoice Number: INV0780Hotel Name: Taste of It...</td>\n",
       "      <td>Food Category - Restaurant Bill/Receipt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>CURRENT Electronic Reservation Slip (ERS)-Norm...</td>\n",
       "      <td>Travel Category - Train Ticket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OCR Contents  \\\n",
       "0     Invoice Number: INV0786Hotel Name: Spice Garde...   \n",
       "1     Invoice No- DW/1112/851166\\n\\nInvoice Date : 2...   \n",
       "2     Hotel: Accor\\r\\nCity: Delhi\\r\\nNumber of Days:...   \n",
       "3     FOOD INN FINE DINE\\nProprietor\\nPER =NNIA REST...   \n",
       "4     = Confirmed Booking Id: NF20115276689422\\n\\nSU...   \n",
       "...                                                 ...   \n",
       "5161  Indiandit\\n\\nSERIAL SNA AUTONOBI\\n\\nr .A.B.Roa...   \n",
       "5162  Base Fare\\nDistance\\nTime\\n\\nSubtotal\\n\\nCHARG...   \n",
       "5163  Dear sanjay kumar,\\n\\nCongratdatons! Thark you...   \n",
       "5164  Invoice Number: INV0780Hotel Name: Taste of It...   \n",
       "5165  CURRENT Electronic Reservation Slip (ERS)-Norm...   \n",
       "\n",
       "                                          Category  \n",
       "0          Food Category - Restaurant Bill/Receipt  \n",
       "1                  Travel Category - Flight Ticket  \n",
       "2     Miscellaneous Category - Miscellaneous Bills  \n",
       "3          Food Category - Restaurant Bill/Receipt  \n",
       "4                  Travel Category - Flight Ticket  \n",
       "...                                            ...  \n",
       "5161             Fuel Category - Fuel Bill/Receipt  \n",
       "5162            Travel Category - Cab Bill/Receipt  \n",
       "5163                Travel Category - Train Ticket  \n",
       "5164       Food Category - Restaurant Bill/Receipt  \n",
       "5165                Travel Category - Train Ticket  \n",
       "\n",
       "[5166 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataStuff = pd.read_csv(\"../augmentedDataStuff.csv\")\n",
    "dataStuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Travel Category - Cab Bill/Receipt',\n",
       " 'Travel Category - Train Ticket',\n",
       " 'Travel Category - Bus Ticket',\n",
       " 'Travel Category - Flight Ticket',\n",
       " 'Food Category - Restaurant Bill/Receipt',\n",
       " 'Electronics Category - Electronic Gadgets Bill/Receipt',\n",
       " 'Room Stay Category - Stay Bill/Receipt',\n",
       " 'Miscellaneous Category - Miscellaneous Bills',\n",
       " 'Fuel Category - Fuel Bill/Receipt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsStuff = list(set(dataStuff[\"Category\"]))\n",
    "clsStuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Travel Category - Cab Bill/Receipt': 6,\n",
       " 'Travel Category - Train Ticket': 8,\n",
       " 'Travel Category - Bus Ticket': 5,\n",
       " 'Travel Category - Flight Ticket': 7,\n",
       " 'Food Category - Restaurant Bill/Receipt': 1,\n",
       " 'Electronics Category - Electronic Gadgets Bill/Receipt': 0,\n",
       " 'Room Stay Category - Stay Bill/Receipt': 4,\n",
       " 'Miscellaneous Category - Miscellaneous Bills': 3,\n",
       " 'Fuel Category - Fuel Bill/Receipt': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(clsStuff)\n",
    "clsDict = dict(zip(clsStuff, labelEncoder.transform(clsStuff)))\n",
    "clsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLabels = len(clsStuff)\n",
    "batchSize = 16\n",
    "learningRate = 2e-5\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2f319324de46da829d280bad34447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f55dccbeda424dbd6dc06368c1b3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18b301574a14a1eb5f0ab6d23a18448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343d91ae2a52499fa9affe4bdc5a9432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0188e169d515486ba348393cbfbc1cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d160156b59f4c218d2a8f796fdfae79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model)\n",
    "wandb.config[\"learning_rate\"] = learningRate\n",
    "wandb.config[\"batch_size\"] = batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4711    Hotel: Hilton Worldwide\\r\\nCity: Amritsar\\r\\nN...\n",
       "4184    goibibo sao\\n\\nFlight Ticket - Kolkata to Agar...\n",
       "1670    Category: Utilities\\r\\nItem: Internet Bill\\r\\n...\n",
       "99      Invoice Number: INV0810Hotel Name: The Hungry ...\n",
       "3592    Invoice Number: INV0953Hotel Name: Gourmet Pal...\n",
       "                              ...                        \n",
       "4426    Invoice Number: INV0558Hotel Name: The Hungry ...\n",
       "466     ticket ric 0: r07s220168 Gcleartrip\\n\\nBangalo...\n",
       "3092    Category: Shopping\\r\\nItem: Accessories\\r\\nAmo...\n",
       "3772    \" ‘\\n\\nSREP iy\\n\\nCHOKHI DHANI PALACE HOTEL\\n\\...\n",
       "860     Category: Healthcare\\r\\nItem: Medicines\\r\\nAmo...\n",
       "Name: OCR Contents, Length: 4132, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData, valData = train_test_split(dataStuff, test_size=0.2, random_state=42)\n",
    "trainData['OCR Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(data):\n",
    "  train_inputs, train_masks, train_labels = [], [], []\n",
    "  for item in zip(data[\"OCR Contents\"], data[\"Category\"]):\n",
    "    text_content, category_label = item\n",
    "    summary_text = f\"{text_content} ({category_label})\"\n",
    "    encoded_data = tokenizer(summary_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    train_inputs.append(encoded_data[\"input_ids\"])\n",
    "    train_masks.append(encoded_data[\"attention_mask\"])\n",
    "    train_labels.append(category_label)\n",
    "  return train_inputs, train_masks, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2Encoding(labelStuff):\n",
    "    encodedLabels = labelEncoder.transform(labelStuff)\n",
    "    return encodedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInputs, trainMasks, trainLabels = prepData(trainData)\n",
    "valInputs, valMasks, valLabels = prepData(valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 3, ..., 3, 1, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels = labels2Encoding(trainLabels)\n",
    "valLabels = labels2Encoding(valLabels)\n",
    "trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInputs = torch.tensor(trainInputs, dtype=torch.uint8).to(device)\n",
    "trainMasks = torch.tensor(trainMasks, dtype=torch.uint8).to(device)\n",
    "trainLabels = torch.tensor(trainLabels, dtype=torch.uint8).to(device)\n",
    "valInputs = torch.tensor(valInputs, dtype=torch.uint8).to(device)\n",
    "valMasks = torch.tensor(valMasks, dtype=torch.uint8).to(device)\n",
    "valLabels = torch.tensor(valLabels, dtype=torch.uint8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 3,  ..., 3, 1, 3], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[234,  10, 243,  ...,   0,   0,   0],\n",
       "        [ 25,  23, 184,  ...,  33, 157,   1],\n",
       "        [ 51,  10, 156,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 51,  10, 238,  ...,   0,   0,   0],\n",
       "        [ 96, 202,   3,  ...,   0,   0,   0],\n",
       "        [ 51,  10, 126,  ...,   0,   0,   0]], device='cuda:0',\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0', dtype=torch.uint8) tensor(0, device='cuda:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(trainMasks.max(), trainInputs.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = TensorDataset(trainInputs, trainMasks, trainLabels)\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataset = TensorDataset(valInputs, valMasks, valLabels)\n",
    "valDataloader = DataLoader(valDataset, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learningRate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, trainDataloader):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  correct = 0\n",
    "  for batch_idx, (input_ids, attention_mask, labels) in enumerate(trainDataloader):\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = input_ids.long().to(device)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Only input_ids and attention_mask needed\n",
    "\n",
    "    loss = criterion(outputs.logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    correct += (predictions == labels).sum().item()\n",
    "  return total_loss / len(trainDataloader), correct / len(trainDataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, valDataloader):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  correct = 0\n",
    "  all_preds, all_labels = [], []\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (input_ids, attention_mask, labels) in enumerate(valDataloader):\n",
    "      input_ids = input_ids.long().to(device)\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      loss = criterion(outputs.logits, labels)\n",
    "      total_loss += loss.item()\n",
    "      predictions = torch.argmax(outputs.logits, dim=1)\n",
    "      correct += (predictions == labels).sum().item()\n",
    "      all_preds.extend(predictions.cpu().numpy())\n",
    "      all_labels.extend(labels.cpu().numpy())\n",
    "  accuracy = accuracy_score(all_labels, all_preds)\n",
    "  precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "  recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "  return total_loss / len(valDataloader), accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"tunedModels\"):\n",
    "   os.makedirs(\"tunedModels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch(s):   0%|\u001b[31m          \u001b[0m| 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(\u001b[38;5;241m2\u001b[39m, colour \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m   train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainDataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m   val_loss, val_acc, val_precision, val_recall \u001b[38;5;241m=\u001b[39m validate(model, criterion, valDataloader)\n\u001b[0;32m      4\u001b[0m   dataDict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Recall\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_recall\n\u001b[0;32m     12\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[41], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, trainDataloader)\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Only input_ids and attention_mask needed\u001b[39;00m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Sid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1740\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1737\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1740\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1008\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m     err_msg_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minput_ids or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.trange(2, colour = \"red\", desc = \"Epoch(s)\"):\n",
    "  train_loss, train_acc = train(model, optimizer, criterion, trainDataloader)\n",
    "  val_loss, val_acc, val_precision, val_recall = validate(model, criterion, valDataloader)\n",
    "  dataDict = {\n",
    "    \"Epoch\": epoch+1,\n",
    "    \"Train Loss\": train_loss,\n",
    "    \"Train Accuracy\": train_acc,\n",
    "    \"Val Loss\": val_loss,\n",
    "    \"Val Accuracy\": val_acc,\n",
    "    \"Val Precision\": val_precision,\n",
    "    \"Val Recall\": val_recall\n",
    "    }\n",
    "  print(dataDict)\n",
    "  #wandb.log(dataDict)\n",
    "  model.save_pretrained(\"tunedModels/trainedT5model - Epoch {}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
