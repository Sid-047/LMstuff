{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"T5 Model FineTuning Yo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStuff = pd.read_csv(\"../augmentedDataStuff.csv\")\n",
    "dataStuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsStuff = list(set(dataStuff[\"Category\"]))\n",
    "clsStuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(clsStuff)\n",
    "clsDict = dict(zip(clsStuff, labelEncoder.transform(clsStuff)))\n",
    "clsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLabels = len(clsStuff)\n",
    "batchSize = 16\n",
    "learningRate = 2e-5\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5Tokenizer.from_pretrained(\"google-t5/t5-small\", num_labels=numLabels).to(device)\n",
    "tokenizer = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model)\n",
    "wandb.config[\"learning_rate\"] = learningRate\n",
    "wandb.config[\"batch_size\"] = batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, valData = train_test_split(dataStuff, test_size=0.2, random_state=42)\n",
    "trainData['OCR Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(data):\n",
    "  train_inputs, train_masks, train_labels = [], [], []\n",
    "  for item, label in zip(data[\"OCR Contents\"], data[\"Category\"]):\n",
    "    text_content = item\n",
    "    encoded_data = tokenizer(text_content, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    train_inputs.append(encoded_data[\"input_ids\"])\n",
    "    train_masks.append(encoded_data[\"attention_mask\"])\n",
    "    train_labels.append(label)\n",
    "  return train_inputs, train_masks, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2Encoding(labelStuff):\n",
    "    encodedLabels = labelEncoder.transform(labelStuff)\n",
    "    return encodedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInputs, trainMasks, trainLabels = prepData(trainData)\n",
    "valInputs, valMasks, valLabels = prepData(valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = labels2Encoding(trainLabels)\n",
    "valLabels = labels2Encoding(valLabels)\n",
    "trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInputs = torch.tensor(trainInputs, dtype=torch.uint8).to(device)\n",
    "trainMasks = torch.tensor(trainMasks, dtype=torch.uint8).to(device)\n",
    "trainLabels = torch.tensor(trainLabels, dtype=torch.uint8).to(device)\n",
    "valInputs = torch.tensor(valInputs, dtype=torch.uint8).to(device)\n",
    "valMasks = torch.tensor(valMasks, dtype=torch.uint8).to(device)\n",
    "valLabels = torch.tensor(valLabels, dtype=torch.uint8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainMasks.max(), trainInputs.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = TensorDataset(trainInputs, trainMasks, trainLabels)\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataset = TensorDataset(valInputs, valMasks, valLabels)\n",
    "valDataloader = DataLoader(valDataset, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learningRate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, trainDataloader):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  correct = 0\n",
    "  for batch_idx, (input_ids, attention_mask, labels) in enumerate(trainDataloader):\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = input_ids.long().to(device)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    loss = criterion(outputs.logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    correct += (predictions == labels).sum().item()\n",
    "  return total_loss / len(trainDataloader), correct / len(trainDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, valDataloader):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  correct = 0\n",
    "  all_preds, all_labels = [], []\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (input_ids, attention_mask, labels) in enumerate(valDataloader):\n",
    "      input_ids = input_ids.long().to(device)\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      loss = criterion(outputs.logits, labels)\n",
    "      total_loss += loss.item()\n",
    "      predictions = torch.argmax(outputs.logits, dim=1)\n",
    "      correct += (predictions == labels).sum().item()\n",
    "      all_preds.extend(predictions.cpu().numpy())\n",
    "      all_labels.extend(labels.cpu().numpy())\n",
    "  accuracy = accuracy_score(all_labels, all_preds)\n",
    "  precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "  recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "  return total_loss / len(valDataloader), accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"tunedModels\"):\n",
    "   os.makedirs(\"tunedModels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm.trange(epochs, colour = \"red\", desc = \"Epoch(s)\"):\n",
    "  train_loss, train_acc = train(model, optimizer, criterion, trainDataloader)\n",
    "  val_loss, val_acc, val_precision, val_recall = validate(model, criterion, valDataloader)\n",
    "  dataDict = {\n",
    "    \"Epoch\": epoch+1,\n",
    "    \"Train Loss\": train_loss,\n",
    "    \"Train Accuracy\": train_acc,\n",
    "    \"Val Loss\": val_loss,\n",
    "    \"Val Accuracy\": val_acc,\n",
    "    \"Val Precision\": val_precision,\n",
    "    \"Val Recall\": val_recall\n",
    "    }\n",
    "  print(dataDict)\n",
    "  wandb.log(dataDict)\n",
    "  model.save_pretrained(\"tunedModels/trainedT5model - Epoch {}\".format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
